/home/rstric2s/psi4conda/envs/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
model:
Sequential(
  (0): Linear(in_features=4, out_features=512, bias=True)
  (1): LeakyReLU(negative_slope=0.01)
  (2): Linear(in_features=512, out_features=2048, bias=True)
  (3): LeakyReLU(negative_slope=0.01)
  (4): Linear(in_features=2048, out_features=64, bias=True)
  (5): LeakyReLU(negative_slope=0.01)
  (6): Linear(in_features=64, out_features=4, bias=True)
  (7): LeakyReLU(negative_slope=0.01)
  (8): Linear(in_features=4, out_features=1, bias=True)
)
num_epochs: 100
learning rate: 0.001


Epoch [1/100], Loss: 12396.7509765625
	Saved PyTorch Model State to model-1_12396.7510.pth
Epoch [2/100], Loss: 11548.7841796875
	Saved PyTorch Model State to model-2_11548.7842.pth
	Removed old PyTorch Model State model-1_12396.7510.pth
Epoch [3/100], Loss: 8999.0654296875
	Saved PyTorch Model State to model-3_8999.0654.pth
	Removed old PyTorch Model State model-2_11548.7842.pth
Epoch [4/100], Loss: 4406.4912109375
	Saved PyTorch Model State to model-4_4406.4912.pth
	Removed old PyTorch Model State model-3_8999.0654.pth
Epoch [5/100], Loss: 1908.8963623046875
	Saved PyTorch Model State to model-5_1908.8964.pth
	Removed old PyTorch Model State model-4_4406.4912.pth
Epoch [6/100], Loss: 155.07119750976562
	Saved PyTorch Model State to model-6_155.0712.pth
	Removed old PyTorch Model State model-5_1908.8964.pth
Epoch [7/100], Loss: 159.68666076660156
Epoch [8/100], Loss: 125.79325866699219
	Saved PyTorch Model State to model-8_125.7933.pth
	Removed old PyTorch Model State model-6_155.0712.pth
Epoch [9/100], Loss: 107.98284912109375
	Saved PyTorch Model State to model-9_107.9828.pth
	Removed old PyTorch Model State model-8_125.7933.pth
Epoch [10/100], Loss: 121.23644256591797
Epoch [11/100], Loss: 80.718994140625
	Saved PyTorch Model State to model-11_80.7190.pth
	Removed old PyTorch Model State model-9_107.9828.pth
Epoch [12/100], Loss: 56.66182327270508
	Saved PyTorch Model State to model-12_56.6618.pth
	Removed old PyTorch Model State model-11_80.7190.pth
Epoch [13/100], Loss: 63.52432632446289
Epoch [14/100], Loss: 60.6048583984375
Epoch [15/100], Loss: 61.874393463134766
Epoch [16/100], Loss: 50.2451171875
	Saved PyTorch Model State to model-16_50.2451.pth
	Removed old PyTorch Model State model-12_56.6618.pth
Epoch [17/100], Loss: 58.91106033325195
Epoch [18/100], Loss: 17.84577751159668
	Saved PyTorch Model State to model-18_17.8458.pth
	Removed old PyTorch Model State model-16_50.2451.pth
Epoch [19/100], Loss: 44.25448989868164
Epoch [20/100], Loss: 38.737403869628906
Epoch [21/100], Loss: 30.84912872314453
Epoch [22/100], Loss: 29.470901489257812
Epoch [23/100], Loss: 37.838565826416016
Epoch [24/100], Loss: 22.67861557006836
Epoch [25/100], Loss: 25.435842514038086
Epoch [26/100], Loss: 37.771018981933594
Epoch [27/100], Loss: 45.62102127075195
Epoch [28/100], Loss: 7.71465539932251
	Saved PyTorch Model State to model-28_7.7147.pth
	Removed old PyTorch Model State model-18_17.8458.pth
Epoch [29/100], Loss: 16.70635986328125
Epoch [30/100], Loss: 18.42905616760254
Epoch [31/100], Loss: 17.014617919921875
Epoch [32/100], Loss: 5.692639350891113
	Saved PyTorch Model State to model-32_5.6926.pth
	Removed old PyTorch Model State model-28_7.7147.pth
Epoch [33/100], Loss: 0.6401758193969727
	Saved PyTorch Model State to model-33_0.6402.pth
	Removed old PyTorch Model State model-32_5.6926.pth
Epoch [34/100], Loss: 5.767443656921387
Epoch [35/100], Loss: 9.462833404541016
Epoch [36/100], Loss: 0.5682826042175293
	Saved PyTorch Model State to model-36_0.5683.pth
	Removed old PyTorch Model State model-33_0.6402.pth
Epoch [37/100], Loss: 0.1716499924659729
	Saved PyTorch Model State to model-37_0.1716.pth
	Removed old PyTorch Model State model-36_0.5683.pth
Epoch [38/100], Loss: 0.5187973380088806
Epoch [39/100], Loss: 70.77590942382812
Epoch [40/100], Loss: 23.42290687561035
Epoch [41/100], Loss: 18.458938598632812
Epoch [42/100], Loss: 38.011470794677734
Epoch [43/100], Loss: 11.631247520446777
Epoch [44/100], Loss: 52.71394729614258
Epoch [45/100], Loss: 37.236515045166016
Epoch [46/100], Loss: 91.80245971679688
Epoch [47/100], Loss: 47.47987365722656
Epoch [48/100], Loss: 98.37458038330078
Epoch [49/100], Loss: 87.18252563476562
Epoch [50/100], Loss: 80.81224060058594
Epoch [51/100], Loss: 78.47663116455078
Epoch [52/100], Loss: 53.42981719970703
Epoch [53/100], Loss: 81.39268493652344
Epoch [54/100], Loss: 126.65040588378906
Epoch [55/100], Loss: 79.80909729003906
Epoch [56/100], Loss: 70.53887939453125
Epoch [57/100], Loss: 77.69135284423828
Epoch [58/100], Loss: 131.4132537841797
Epoch [59/100], Loss: 196.41722106933594
Epoch [60/100], Loss: 136.59194946289062
Epoch [61/100], Loss: 81.58663177490234
Epoch [62/100], Loss: 120.13011169433594
Epoch [63/100], Loss: 116.22087860107422
Epoch [64/100], Loss: 119.84529876708984
Epoch [65/100], Loss: 179.79299926757812
Epoch [66/100], Loss: 132.691162109375
Epoch [67/100], Loss: 80.68719482421875
Epoch [68/100], Loss: 126.15084838867188
Epoch [69/100], Loss: 102.38465118408203
Epoch [70/100], Loss: 107.05757141113281
Epoch [71/100], Loss: 154.85086059570312
Epoch [72/100], Loss: 81.09780883789062
Epoch [73/100], Loss: 95.77914428710938
Epoch [74/100], Loss: 98.54779052734375
Epoch [75/100], Loss: 69.15550994873047
Epoch [76/100], Loss: 147.09408569335938
Epoch [77/100], Loss: 126.38265991210938
Epoch [78/100], Loss: 66.27474212646484
Epoch [79/100], Loss: 94.92805480957031
Epoch [80/100], Loss: 95.06011962890625
Epoch [81/100], Loss: 61.10002517700195
Epoch [82/100], Loss: 64.35204315185547
Epoch [83/100], Loss: 163.17630004882812
Epoch [84/100], Loss: 111.84508514404297
Epoch [85/100], Loss: 123.53758239746094
Epoch [86/100], Loss: 24.35480308532715
Epoch [87/100], Loss: 64.5931625366211
Epoch [88/100], Loss: 40.65074157714844
Epoch [89/100], Loss: 27.043935775756836
Epoch [90/100], Loss: 25.076963424682617
Epoch [91/100], Loss: 103.61734008789062
Epoch [92/100], Loss: 74.84931945800781
Epoch [93/100], Loss: 62.34193420410156
Epoch [94/100], Loss: 43.6281852722168
Epoch [95/100], Loss: 28.04912567138672
Epoch [96/100], Loss: 51.388526916503906
Epoch [97/100], Loss: 20.237367630004883
Epoch [98/100], Loss: 51.218902587890625
Epoch [99/100], Loss: 101.9207534790039
Epoch [100/100], Loss: 24.32107925415039
