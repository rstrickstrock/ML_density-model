/home/rstric2s/psi4conda/envs/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
Epoch [1/100], Loss: 19156.966796875
Epoch [2/100], Loss: 13184.76953125
Epoch [3/100], Loss: 11107.75
Epoch [4/100], Loss: 9601.236328125
Epoch [5/100], Loss: 8482.9111328125
Epoch [6/100], Loss: 7631.724609375
Epoch [7/100], Loss: 7001.76904296875
Epoch [8/100], Loss: 6507.5966796875
Epoch [9/100], Loss: 6122.43212890625
Epoch [10/100], Loss: 5810.45166015625
Epoch [11/100], Loss: 5545.5126953125
Epoch [12/100], Loss: 5325.42724609375
Epoch [13/100], Loss: 5128.33984375
Epoch [14/100], Loss: 4950.27001953125
Epoch [15/100], Loss: 4785.5
Epoch [16/100], Loss: 4677.23583984375
Epoch [17/100], Loss: 4594.802734375
Epoch [18/100], Loss: 4517.53173828125
Epoch [19/100], Loss: 4445.79833984375
Epoch [20/100], Loss: 4389.48291015625
Epoch [21/100], Loss: 4341.64697265625
Epoch [22/100], Loss: 4309.7333984375
Epoch [23/100], Loss: 4285.6376953125
Epoch [24/100], Loss: 4277.18701171875
Epoch [25/100], Loss: 4280.0458984375
Epoch [26/100], Loss: 4294.392578125
Epoch [27/100], Loss: 4315.17626953125
Epoch [28/100], Loss: 4338.6552734375
Epoch [29/100], Loss: 4361.50439453125
Epoch [30/100], Loss: 4376.8349609375
Epoch [31/100], Loss: 4386.3212890625
Epoch [32/100], Loss: 4388.3671875
Epoch [33/100], Loss: 4387.849609375
Epoch [34/100], Loss: 4382.8701171875
Epoch [35/100], Loss: 4381.77880859375
Epoch [36/100], Loss: 4393.794921875
Epoch [37/100], Loss: 4401.4931640625
Epoch [38/100], Loss: 4401.29052734375
Epoch [39/100], Loss: 4396.5302734375
Epoch [40/100], Loss: 4386.9599609375
Epoch [41/100], Loss: 4373.0888671875
Epoch [42/100], Loss: 4360.68212890625
Epoch [43/100], Loss: 4344.2451171875
Epoch [44/100], Loss: 4323.6083984375
Epoch [45/100], Loss: 4301.84326171875
Epoch [46/100], Loss: 4278.560546875
Epoch [47/100], Loss: 4252.171875
Epoch [48/100], Loss: 4222.2470703125
Epoch [49/100], Loss: 4191.1083984375
Epoch [50/100], Loss: 4156.2197265625
Epoch [51/100], Loss: 4119.18896484375
Epoch [52/100], Loss: 4076.546630859375
Epoch [53/100], Loss: 4032.653076171875
Epoch [54/100], Loss: 3986.908203125
Epoch [55/100], Loss: 3940.144287109375
Epoch [56/100], Loss: 3890.160888671875
Epoch [57/100], Loss: 3838.734130859375
Epoch [58/100], Loss: 3785.71923828125
Epoch [59/100], Loss: 3730.9326171875
Epoch [60/100], Loss: 3655.945556640625
Epoch [61/100], Loss: 3597.1513671875
Epoch [62/100], Loss: 3533.193603515625
Epoch [63/100], Loss: 3467.2431640625
Epoch [64/100], Loss: 3397.757080078125
Epoch [65/100], Loss: 3325.636962890625
Epoch [66/100], Loss: 3252.111083984375
Epoch [67/100], Loss: 3178.815185546875
Epoch [68/100], Loss: 3104.66064453125
Epoch [69/100], Loss: 3027.60546875
Epoch [70/100], Loss: 2950.2919921875
Epoch [71/100], Loss: 2873.552734375
Epoch [72/100], Loss: 2792.24267578125
Epoch [73/100], Loss: 2691.876953125
Epoch [74/100], Loss: 2608.519775390625
Epoch [75/100], Loss: 2497.7177734375
Epoch [76/100], Loss: 2407.452392578125
Epoch [77/100], Loss: 2308.244140625
Epoch [78/100], Loss: 2208.558349609375
Epoch [79/100], Loss: 2109.8388671875
Epoch [80/100], Loss: 2013.7491455078125
Epoch [81/100], Loss: 1915.066650390625
Epoch [82/100], Loss: 1823.6142578125
Epoch [83/100], Loss: 1710.507080078125
Epoch [84/100], Loss: 1616.487548828125
Epoch [85/100], Loss: 1519.1058349609375
Epoch [86/100], Loss: 1416.8258056640625
Epoch [87/100], Loss: 1320.5665283203125
Epoch [88/100], Loss: 1231.9654541015625
Epoch [89/100], Loss: 1144.8458251953125
Epoch [90/100], Loss: 1062.9017333984375
Epoch [91/100], Loss: 984.8923950195312
Epoch [92/100], Loss: 907.6774291992188
Epoch [93/100], Loss: 841.1309814453125
Epoch [94/100], Loss: 777.7099609375
Epoch [95/100], Loss: 720.7769775390625
Epoch [96/100], Loss: 669.1903076171875
Epoch [97/100], Loss: 618.27685546875
Epoch [98/100], Loss: 573.287353515625
Epoch [99/100], Loss: 531.538330078125
Epoch [100/100], Loss: 494.953857421875
